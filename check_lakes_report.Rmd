---
title: "Checking the progress of site analysis"
author: "Simon Goring"
date: "July 16, 2017"
output:
  html_document:
    self_contained: true
    code_folding: hide
    toc: true
    toc_float: true
    css: css/sjg_markdown.css
---

## Status of Lake Records in Neotoma

Neotoma contains records from a number of lakes, some of which have reported areas, many from legacy publications. In addition to missing areas, examination of site coordinates indicates that a large number of records have rounded coordinate values which may not intersect with the waterbodies from which sedimentary or water chemistry records were obtained.

## Load in the records

Bailey Zak and Claire Rubblike have been processing a number of lake sites from Neotoma, to align them with existing records in the Canadian and United States Hydrological Databases.  This provides an important service, it allows us to check reported lake areas & deposition types as well as providing an opportunity to correct geopositioning for sites that were reported with rounding for latitude or longitude coordinates.

### Set up

The first step was time-consuming.  All pollen cores from Canada and the United States were pulled from Neotoma, along with the chronological controls for each record.  The chronological controls were used to help prioritize reconstruction, since the current effort is largely focused on the reconstruction of "high quality" sites.

The result here is a `data.frame` called `control_output` that contains each site id, dataset id and a sum of all the chron control types used in reconstruction.  This is for every pollen record (regardless of whether it's a lake or not) in Canada and the United States.

```{r lakedata, cache=FALSE, message = FALSE, warning=FALSE, results='hide'}

library(purrr)
library(dplyr)
library(reshape2)
library(sf)
library(leaflet)

version <- '1.4'

all_ds <- neotoma::bind(neotoma::get_dataset(datasettype = 'pollen',
                                             gpid = 'Canada'),
                        neotoma::get_dataset(datasettype = 'pollen',
                                             gpid = 'United States'))

if (!paste0('version_', version) %in% list.files('data')) {
  dir.create(paste0('data/version_', version))
}

if (!paste0('chron_control_status_version_', version, '.csv') %in% list.files(paste0('data/version_',version))) {

  all_ds <- neotoma::bind(neotoma::get_dataset(datasettype = 'pollen',
                                               gpid = 'Canada'),
                          neotoma::get_dataset(datasettype = 'pollen',
                                               gpid = 'United States'))

  controls <- all_ds %>% map(function(x)try(neotoma::get_chroncontrol(x)))

  chron_table <- list()

  for(i in 1:length(all_ds)) {
    if(length(controls[[i]][[1]]) == 1 | 
       'try-error' %in% class(controls[[i]]) |
       all(is.na(controls[[i]][[1]]$chron.control))) {
      chron_table[[i]] <- data.frame(dsid = all_ds[[i]]$dataset$dataset.meta$dataset.id,
                                     stid = all_ds[[i]]$dataset$site.data$site.id)
    } else {

      type_count <- controls[[i]][[1]]$chron.control %>%
        group_by(control.type) %>%
        summarise(n = n()) %>% 
        mutate(control.type = as.character(control.type))
      
      if (any(is.na(type_count$control.type))) {
        type_count$control.type[is.na(type_count$control.type)] <- "No control"
      }

      if (all(is.na(diff(controls[[i]][[1]]$chron.control$age)))) {
        max_interval <- -999
        avg_interval <- -999
      } else {
        avg_interval <- mean(diff(controls[[i]][[1]]$chron.control$age), na.rm = TRUE)
        max_interval <-  max(diff(controls[[i]][[1]]$chron.control$age), na.rm = TRUE)
      }

      chron_table[[i]] <- data.frame(i = i,
                                     dsid = all_ds[[i]]$dataset$dataset.id,
                                     stid = all_ds[[i]]$site.data$site.id,
                                     controls = sum(type_count$n, na.rm=TRUE),
                                     type_count,
                                     avg_int = avg_interval,
                                     max_int = max_interval,
                                     stringsAsFactors = FALSE) %>%
        dcast(i + dsid + stid + avg_int + max_int + controls ~ control.type, 
              value.var = 'n')

    }
  }

  control_output <- chron_table %>% bind_rows()
  
  control_output[is.na(control_output)] <- 0

  readr::write_csv(control_output,
                   paste0('data/version_',version,'/chron_control_status_version_', version, '.csv'))
} else {
  control_output <- readr::read_csv(paste0('data/version_',
                                           version,
                                           '/chron_control_status_version_', version, '.csv'))
}

```

This code block provides a list of all pollen datasets in Neotoma along with the chronological controls used to develop their chronologies.  It results in a data frame containing `r nrow(control_output)` rows and `ncol(control_output)` columns.

A secondary process is then run across all lakes in the Canadian [CanVec â€“ Hydro Features](http://open.canada.ca/data/en/dataset/8ba2aa2a-7bb9-4448-b4d7-f164409fe056) and the [United States Hydrolography Dataset](https://nhd.usgs.gov/NHD_High_Resolution.html) to obtain "best fit" matches for lakes. The script, which uses the [`sp` package](https://cran.r-project.org/web/packages/sp/index.html) to overlay the points over lake polygons is contained in the files `R/GetLakeAreas_usa.R` and `R/GetLakeAreas_canada.R`.  State-level shapefles are very large for some states, and as such this is a time and resource consuming process.  We run this step on the Pensylvannia State University's Center for Environmental Informatics server.  The result is a `data.frame` with columns that are indexed using the `stid`.

To do this we have to use `scp`, or secure copy, to move the file over onto the Penn State server and then run the script using `Rscript -e 'GetLakeAreas_usa.R`.  This provides us with the unique identifer of any pollen sites that overlay lakes in the hydrological datasets, and provides matches for any similarly named lakes proximate to a Neotoma lake.

#### Direct Matches

Direct matches occur when a Neotoma site lies directly over top of a record in the Canadian or United States NHD.  These records are tagged with the unique identifier for the lake. Here records with positive matches are indicated in red and those without are indicated in black:

```{r, loadLakeData, echo=FALSE, message=FALSE, results='hide'}

ca_lakes <- readr::read_csv('data/ca_lakes.csv')
us_lakes <- readr::read_csv('data/usa_lakes.csv')

recorded_lakes <- data.frame(site.name = c(ca_lakes$site.name, us_lakes$site.name),
                             dsid      = c(ca_lakes$DatasetID, rep(NA, nrow(us_lakes))),
                             stid      = c(ca_lakes$SiteID, us_lakes$SiteID),
                             lat       = c(ca_lakes$lat, us_lakes$lat),
                             long      = c(ca_lakes$long, us_lakes$long),
                             linked    = c(!is.na(ca_lakes$feature_id),
                                                  !is.na(us_lakes$GNIS_ID)),
                             oarea     = c(ca_lakes$lake_area_ha, us_lakes$AREAHA))

linked_lakes <- recorded_lakes %>% 
  filter(long < 0 & linked == TRUE) %>% 
  distinct()

recorded_lakes <- recorded_lakes %>% filter(long < 0)

recorded_lakes$color   <- ifelse(recorded_lakes$linked, 'blue', 'red')

recorded_lakes$opactiy <- c(0.6, 0.2)[match(recorded_lakes$color, c('red', 'black'))]
recorded_lakes$popup   <- paste0('<b>',recorded_lakes$site.name,'</b><hr>',
                                 '<a href=http://apps.neotomadb.org/explorer/?siteids=',
                                 recorded_lakes$stid,
                                 '>Explorer Link</a>')

leaflet(recorded_lakes)  %>%
  addProviderTiles('Stamen.TonerLite') %>%
  addCircleMarkers(lng = ~long, lat = ~lat, fillColor = ~color,
                   fillOpacity = recorded_lakes$opacity,
                   radius = 3,
                   stroke = FALSE,
                   popup = ~popup) %>%
  addLegend("bottomright",
            colors = c('red', 'blue'),
            labels = c('Linked', 'Not Linked'),
    title = "Linked To Hydro Database",
    opacity = 1)  

```

Of the `r nrow(ca_lakes)` lakes in Canada, a total of `r sum(!is.na(ca_lakes$lake_area_ha))` were matched directly to lakes within the hydrographic database.  Of the `r nrow(us_lakes)` reported from the United States, there were `r sum(!is.na(us_lakes$AREAHA))` with matches to the hydrographic database.

### Reanalysis

<img src="images/map_schema.svg" style="float:right;" width="200px">

From here we passed all lakes without direct matches to a workflow in which individuals examined regional maps, the original publications and secondary information (including Alwynne Beaudoin's [Lake Files](http://www.scirpus.ca/lakes/lakes.php)) to determine whether a site had a match with a nearby lake or other depositional environment.

For all sites we examined location and then either edited the site coordinates or left the site in place, recording the decision using a numeric key and a descriptive element:

| Value | Interpretation |
| ----- | -------------- |
| 0     | This is a data artifact from ArcGIS editing, which assigned a 0 to records edited using ArcMap |
| 1     | Site located and moved.  |
| 2     | Site is in the right place, not moved. |
| 3     | Site not moved, no reasonable match. |

All lakes that have been edited are diplayed here.

```{r, loadData, message=FALSE, warning=FALSE, results='hide'}

source('R/load_entered_data.R')

linked_lakes <- linked_lakes %>% 
  mutate(edited = 2,
         notes = "Compiled from Hydrological database.") %>% 
  rename(`area` = `oarea`)

edited_lakes <- suppressMessages(load_lakes()) %>%
  dplyr::full_join(linked_lakes) %>% 
  mutate(area = round(area, 1)) %>% 
  filter(!is.na(area) & area > 0) %>% 
  distinct(stid, area, .keep_all = TRUE)

dup_sites <- edited_lakes$stid[duplicated(edited_lakes$stid)] %>% unique

hydro_set <- edited_lakes$notes == "Compiled from Hydrological database."

# There are currently some duplicate sites, these don't vary too much, so 
# we take either the value associated with the hydrological database, or 
# the first of the two entered into the database.

for (i in dup_sites) {
  
  set <- edited_lakes$stid == i
  hydro_set <- edited_lakes$notes == "Compiled from Hydrological database."
  
  if(any(hydro_set[set])) {
    drop <- which(set & !hydro_set)
    edited_lakes <- edited_lakes[-drop, ]
  } else{
    edited_lakes <- edited_lakes[-which(set)[2], ]
  }
  
}

sitedataset <- data.frame(stid = sapply(all_ds, function(x) x$site.data$site.id),
                          dsid = sapply(all_ds, function(x) x$dataset.meta$dataset.id),
                          name = sapply(all_ds, function(x) x$site.data$site.name))

edited_lakes$dsid <- sitedataset$dsid[match(edited_lakes$stid, sitedataset$stid)]
coords <- st_coordinates(edited_lakes$geometry) %>% as.data.frame

edited_lakes$lat[is.na(edited_lakes$lat)] <- coords$Y[is.na(edited_lakes$lat)]
edited_lakes$long[is.na(edited_lakes$long)] <- coords$X[is.na(edited_lakes$long)]
edited_lakes$site.name <- sitedataset$name[match(edited_lakes$stid, sitedataset$stid)]

```

This results in a dataset of `r nrow(edited_lakes)` with areas, of which `r sum(edited_lakes$edited %in% c(1,2))` were matched to reported locations.  A total of `r sum(edited_lakes$edited == 1)` lake positions were moved, `r sum(edited_lakes$edited == 3)` could not be properly positioned.  There remain `r sum(!sitedataset$stid %in% edited_lakes$stid)` lakes to work through.

```{r}

edited_lakes$popup <- paste0(edited_lakes$site.name,
                             '<br>(<b>',
                             edited_lakes$edited, '</b>) - ',
                             'New Area: ',
                             edited_lakes$area)

edited_lakes <- edited_lakes %>%
  filter(!edited == 0 | is.na(lat) | is.na(long)) %>%
  st_sf(sf_column_name = "geometry")

pal <- colorFactor(
  palette = "viridis",
  domain = factor(edited_lakes$edited))

leaflet(edited_lakes) %>%
  addTiles() %>%
  addCircleMarkers(lat = ~lat,
                   lng = ~long,
                   color = ~pal(factor(edited)),
                   popup = ~popup) %>%
  addLegend("bottomright", pal = pal, values = ~factor(edited),
    title = "Edit Status",
    opacity = 1)

```

This results in a dataset of `r nrow(edited_lakes)`, of which `r sum(edited_lakes$edited %in% c(1,2))` were matched to reported locations, and `r sum(edited_lakes$area > 0 | !is.na(edited_lakes$oarea), na.rm = TRUE)` lakes have recorded areas.  A total of `r sum(edited_lakes$edited == 1)` lake positions were moved, `r sum(edited_lakes$edited == 3)` could not be properly positioned.  There remain `r sum(!unique(recorded_lakes$stid) %in% unique(edited_lakes$stid))` lakes to work through.

```{r, generateMissing, echo = FALSE, warning=FALSE, results='hide'}

new_output <- data.frame(stid = c(us_lakes$SiteID, ca_lakes$SiteID),
                         name = c(us_lakes$site.name, ca_lakes$site.name),
                         long = c(us_lakes$long,   ca_lakes$long),
                          lat = c(us_lakes$lat,    ca_lakes$lat),
                         area = c(us_lakes$AREAHA, ca_lakes$lake_area_ha)) %>%
  dplyr::left_join(control_output, by = 'stid')

new_output <- new_output %>%
  filter(!stid %in% edited_lakes$stid) %>%
  filter(long < 0 & !is.na(long)) %>%
  arrange(controls) %>%
  distinct(stid, .keep_all = TRUE) %>%
  st_as_sf(coords = c('long', 'lat'), crs = 4326)

  new_output$area <- ''
new_output$edited <- ''
 new_output$notes <- ''
  new_output$type <- ''

new_output <- new_output %>%
  select(stid, dsid, name, edited, notes, type, area, avg_int, max_int, controls, geometry)

leaflet(new_output) %>%
  addTiles() %>%
  addCircleMarkers(popup = ~name)

sf::write_sf(new_output, 
             paste0('data/version_',version,'/dataset_',version,'.shp'), 
             delete_layer = TRUE)

```

## Looking at changes

### Changes in Location

```{r, locationChangeCSVOutput, echo = FALSE, warning=FALSE}

sites <- neotoma::get_site(all_ds)

align <- match(edited_lakes$stid, sites$site.id)

try_lakes <- edited_lakes %>% 
  as.data.frame %>% 
  select(stid, dsid, site.name, lat, long, edited, notes, area, type) %>% 
  mutate(delta = round(sqrt((sites$lat[align] - lat)^2 + (sites$long[align] - long)^2), 3)) %>% 
  mutate(popup = paste0('<b>', site.name, '</b>',
                        '<br><b>Lake Area:</b> ', area,
                        '<br><b>Distance Moved:</b> ', delta,
                        '<br><a href=http://apps.neotomadb.org/explorer/?siteids=',
                        stid,'>Explorer Link</a>'))

pal <- colorNumeric(
  palette = "viridis",
  domain = try_lakes$delta)

leaflet(try_lakes %>% filter(delta > 0)) %>%
  addTiles() %>%
  addCircleMarkers(lat = ~lat,
                   lng = ~long,
                   color = ~pal(delta),
                   popup = ~popup,
                   radius = 5) %>%
    addLegend("bottomright", pal = pal, values = ~delta,
    title = "Distance Moved",
    opacity = 1)

out_lakes <- try_lakes %>%
  arrange(desc(delta)) %>%
  select(-popup)

out_lakes$type[out_lakes$type %in% c('Lac', 'Lacu', 'lAC', 'Lac and Pal')] <- 'Lacu'
out_lakes$type[out_lakes$type %in% c('Pal', 'Palu')] <- 'Palu'

readr::write_csv(out_lakes,
                 path = paste0('data/version_', version, '/area_lakes_',version, '.csv'))

```

A total of `r sum(try_lakes$delta > 0, na.rm = TRUE)` records have changed locations.  Of these lakes `r sum(try_lakes$delta > 0 & (!is.na(try_lakes$oarea) | !is.na(try_lakes$area)), na.rm = TRUE)` have both moved coordinates and reported areas.  A total of `r sum((!is.na(try_lakes$oarea) | !is.na(try_lakes$area)), na.rm = TRUE)` lakes have reported area.

### Editing Categories:

```{r, editCategories, echo = FALSE, warning=FALSE}

edited_lakes <- edited_lakes %>% st_sf(., sf_column_name = "geometry") %>%
  filter(!is.na(edited) | !edited == 0)

pal <- colorFactor(
  palette = "viridis",
  domain = edited_lakes$edited)

leaflet(edited_lakes) %>%
  addTiles() %>%
  addCircleMarkers(fillColor = ~pal(edited),
                   color =  ~pal(edited),
                   radius = 3) %>%
  addLegend("bottomright", pal = pal, values = ~edited,
    title = "Edit Class",
    opacity = 1)

```

These are the editing categories.
