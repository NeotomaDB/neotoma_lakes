---
title: "US Lake sizes in Neotoma"
author: "Simon Goring"
---

This is a rewrite of the original document, that attempts to match Neotoma sites with their associated lakes within an RMarkdown document for more efficient review and analysis.

```{r}

library(rgdal)
library(neotoma)
library(dplyr)
library(purrr)
library(sp)
library(datasets)
library(sf)

```

For this work we download Neotoma datasets using the `neotoma` R package.  Datasets are downloaded from Neotoma state by state, so that the Neotoma state level data intersects with the US National Hydrology database.

The code to align lakes with their associated Neotoma sites can be found in `R/us_state_lakes.R`.

```{r}
source('R/us_state_lakes.R')
```

`us_state_lakes()` accesses shapefiles from the National Hydrography Dataset for each individual state in the United States.  The function uses the `sf` package to open the shapefiles, perform an intersection between the Neotoma points and the shapefiles, and then returns information about the lakes with which each Neotoma site intersects.

The function also looks for any lakes within the state that have the same name as the Neotoma site.

### Executing the Function

The script uses the entire set of US state abbreviations.  Because the script takes a significant amount of time, and the downloads are quite large, this script is run through a loop, saving the interim output of the `runs` list using a date-time stamped `rds` file.

```{r}

# state.abb is a data element in R for US state abbreviations.
states <- state.abb

data_run <- Sys.Date()

if (file.exists(paste0("data/interim/runs_", data_run, ".rds"))) {
  runs <- readRDS(paste0("data/interim/runs_", data_run, ".rds"))
} else {
  runs <- list()
}

for (i in 1:length(states)) {
  runs[[i]] <- try(state_output(states[i]))
  saveRDS(runs, paste0("runs", Sys.Date(), ".rds"))

  # Clean temprary files:
  file.remove(list.files(tempdir(), full.names = TRUE, recursive = TRUE))
  file.remove(list.files("Shape", full.names = TRUE, recursive = TRUE))
  gc()
}

for (i in length(runs):1) {
  # clean out the outputs that errored.
  if ("try-error" %in% class(runs)) {
    runs[[i]] <- NULL
  }
}

areas <- bind_rows(runs)

areas_clean <- areas %>%
  dplyr::select(stid, `dsid` = `DatasetID`, site.name,
                long, lat, state, GNIS_NAME, GNIS_ID, AREASQKM,
                dist_match, data_source)

areas_clean$AREAHA <- areas_clean$AREASQKM * 100

matched <- is.na(areas_clean$GNIS_ID)

readr::write_csv(areas_clean, "data/output/all_usa_pollensites.csv")
readr::write_csv(areas_clean[matched, ],
                 "data/output/NHD_usa_pollensites.csv")
readr::write_csv(areas_clean[!matched,],
                 "data/output/noNHD_usa_pollensites.csv")

```

From this we have a total of `r nrow(areas_clean)` datasets within Neotoma that can be directly aligned with the National Hydrology Dataset.  We can use these to modify the Neotoma metadata as well.

```{r}
DT::datatable(areas_clean)
```

## Matched Lakes

```{r}

areas_clean$info <- paste0("<b>", areas_clean$site.name, "</b><br>",
                           "Link: <a href=apps.neotomadb.org/explorer/?dsid=", areas_clean$dsid, ">Explorer</a>")

map <- leaflet() %>%
  addProviderTiles("Esri.WorldImagery") %>%
  addMarkers(lat = areas_clean$lat[matched],
             lng = areas_clean$long[matched],
             clusterOptions = markerClusterOptions(),
             popup = areas_clean$info[matched])
```

## Unmatched Lakes
```{r}
map <- leaflet() %>%
  addProviderTiles("Esri.WorldImagery") %>%
  addMarkers(lat = areas_clean$lat[!matched],
             lng = areas_clean$long[!matched],
             clusterOptions = markerClusterOptions(),
             popup = areas_clean$info[!matched])
```
