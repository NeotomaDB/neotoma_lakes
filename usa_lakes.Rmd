---
title: "US Lake sizes in Neotoma"
author: "Simon Goring"
---

This is a rewrite of the original document, that attempts to match Neotoma sites with their associated lakes within an RMarkdown document for more efficient review and analysis.

```{r}

library(rgdal)
library(neotoma)
library(dplyr)
library(purrr)
library(sp)
library(datasets)
library(sf)

```

For this work we download Neotoma datasets using the `neotoma` R package.  Datasets are downloaded from Neotoma state by state, so that the Neotoma state level data intersects with the US National Hydrology database.

```{r}

state_output <- function(x, lake_table = NULL) {

  state <- datasets::state.name[match(x, state.abb)]

  dset <- neotoma::get_dataset(gpid = state, datasettype = "pollen")

  if (!length(dset) > 0) {
    # Exit the function if there's no data for that state.
    return(data.frame(state = paste0(state, "NoData")))
  }

  pol <- dset %>% neotoma::get_site()

  class(pol) <- "data.frame"

  pol <- pol %>%
    mutate(dsid = sapply(dset, function(x) { x$dataset.meta$dataset.id })) %>%
    dplyr::rename(`stid` = `site.id`)

  # Have lakes already been run?
  data_instate <- !is.na(match(pol$stid, lake_table$stid))

  if (any(data_instate)) {

    matches <- match(pol$site.id, lake_table$stid)

    pol$long <- lake_ds$long[matches]
    pol$lat  <- lake_ds$lat[matches]

  }

  # There are a couple states where we get an abnormally large return.

  if (length(dset) < 2774 & length(dset) > 0) {
    temp <- tempfile()

    state <- gsub(" ", "_", state)

    base_uri <- paste0("http://prd-tnm.s3-website-us-west-2.amazonaws.com/StagedProducts",
                       "/Hydrography/NHD/State/HighResolution/Shape/NHD_H_", state, "_Shape.zip")

    test_dl <- try(download.file(base_uri, temp))

    if ("try-error" %in% class(test_dl)) {
      return(cbind(data.frame(GNIS_ID = NA, pol)))
    }

    data <- unzip(temp, overwrite = TRUE)

    pol_sf <- st_as_sf(pol,
                       coords = c("long", "lat"),
                       crs = 4326)

    state_data <- st_read(data[grep("NHDWaterbody.shp", data)]) %>%
      st_transform(crs = 4326)

    get_polydata <- function(x) {
      if (length(x) == 0) {
        return(data.frame(GNIS_ID = NA))
      } else {
        new_text <- st_as_text(state_data$geometry[x], EWKT = TRUE)

        output <- state_data[x,] %>%
          as.data.frame %>%
          select(-geometry)

        output$geometry <- new_text
        return(output)
      }
    }

    get_output <- st_within(pol_sf, state_data) %>%
      map(get_polydata) %>%
      bind_rows

    get_output_pol <- cbind(get_output, pol)

    get_output_pol$state <- x
    get_output_pol$data_source <- base_uri

    nameMatch <- match(pol$site.name, state_data$GNIS_NAME)

    if (any(!is.na(nameMatch))) {

      get_output_pol$match_area <- get_output_pol$dist_match <- NA

      gdmatch <- !is.na(nameMatch)

      get_output_pol$dist_match[gdmatch] <- diag(st_distance(x = st_transform(pol_sf[gdmatch,],
                                                                                        crs = 2163),
                                                                       y = st_transform(state_data[na.omit(nameMatch),],
                                                                                        crs = 2163))) / 1000

      get_output_pol$match_area[gdmatch] <- state_data$AREASQKM[na.omit(nameMatch)]

    }

    return(get_output_pol)
  } else {
    return (data.frame(GNIS_ID = NA))
  }
}

```

Given this function, run the code:

```{r}

# state.abb is a data element in R for US state abbreviations.
states <- state.abb

data_run <- Sys.Date()

if (file.exists(paste0("data/interim/runs_", data_run, ".rds"))) {
  runs <- readRDS(paste0("data/interim/runs_", data_run, ".rds"))
}

for (i in 1:length(states)) {
  runs[[i]] <- try(state_output(states[i]))
  saveRDS(runs, paste0("runs", Sys.Date(), ".rds"))

  # Clean temprary files:
  file.remove(list.files(tempdir(), full.names = TRUE, recursive = TRUE))
  file.remove(list.files("Shape", full.names = TRUE, recursive = TRUE))
  gc()
}

for (i in length(runs):1) {
  if ("try-error" %in% class(runs)) {
    runs[[i]] <- NULL
  }
}

areas <- bind_rows(runs)

areas_clean <- areas %>%
  dplyr::select(stid, `dsid` = `DatasetID`, site.name,
                long, lat, state, GNIS_NAME, GNIS_ID, AREASQKM,
                dist_match, data_source)

areas_clean$AREAHA <- areas_clean$AREASQKM * 100

matched <- is.na(areas_clean$GNIS_ID)

readr::write_csv(areas_clean, "data/output/all_usa_pollensites.csv")
readr::write_csv(areas_clean[matched, ],
                 "data/output/NHD_usa_pollensites.csv")
readr::write_csv(areas_clean[!matched,],
                 "data/output/noNHD_usa_pollensites.csv")

```

From this we have a total of `r nrow(areas_clean)` datasets within Neotoma that can be directly aligned with the National Hydrology Dataset.  We can use these to modify the Neotoma metadata as well.

```{r}
DT::datatable(areas_clean)
```

## Matched Lakes

```{r}

areas_clean$info <- paste0("<b>", areas_clean$site.name, "</b><br>",
                           "Link: <a href=apps.neotomadb.org/explorer/?dsid=", areas_clean$dsid, ">Explorer</a>")

map <- leaflet() %>%
  addProviderTiles("Esri.WorldImagery") %>%
  addMarkers(lat = areas_clean$lat[matched],
             lng = areas_clean$long[matched],
             clusterOptions = markerClusterOptions(),
             popup = areas_clean$info[matched])
```

## Unmatched Lakes
```{r}
map <- leaflet() %>%
  addProviderTiles("Esri.WorldImagery") %>%
  addMarkers(lat = areas_clean$lat[!matched],
             lng = areas_clean$long[!matched],
             clusterOptions = markerClusterOptions(),
             popup = areas_clean$info[!matched])
```
